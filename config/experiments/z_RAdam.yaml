scheduler:
  batch_step: False
  name: MultiStepLR
  milestones: [1000]
  gamma: 0.2
  pct_start: 0.1
  steps_per_epoch: 1

optimizer:
  learning_rate: 0.001
  name: RAdam
  weight_decay: 0.0001 # 0 # 0.00005
  momentum: None